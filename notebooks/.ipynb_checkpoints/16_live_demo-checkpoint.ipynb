{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing News API Real Time Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls 10 articles from each of our 4 news categories from the bing news api\n",
    "subscriptionKey = os.environ['BING_SEARCH_V7_SUBSCRIPTION_KEY']\n",
    "endpoint = \"https://api.bing.microsoft.com/v7.0/news\"\n",
    "\n",
    "# Construct a request\n",
    "query = \"\"\n",
    "categories = [\"World\", \"Business\", \"Sports\", \"Science\"]\n",
    "count = 100\n",
    "freshness = \"Day\"\n",
    "mkt = 'en-US'\n",
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json'))\n",
    "\n",
    "for category in categories:\n",
    "    params = {'q': query, 'mkt': mkt, 'category': category, 'count': count, 'freshness': freshness}\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscriptionKey}\n",
    "    file_name = datetime.today().strftime('%Y%m%d') + \"_\" + category\n",
    "    \n",
    "    # Call the API\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        #write json string to file\n",
    "        with open(os.path.join(data_file_path,file_name+'.json'), 'w') as json_file:\n",
    "          json.dump(response.json(), json_file)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "    time.sleep(1)\n",
    "    #free account offers only 3 requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import string\n",
    "from num2word import word\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[wizard, complete, turnaround, rout, pacer, cl...</td>\n",
       "      <td>[six, week, falling, one, thousand, seven, hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[falcon, rookie, kyle, pitt, invited, tight, e...</td>\n",
       "      <td>[falcon, rookie, tight, end, kyle, pitt, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[albert, pujols, go, deep, first, home, run, d...</td>\n",
       "      <td>[solid, start, dodger, blue, 10time, allstar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[felipe, vazquez, former, pittsburgh, pirate, ...</td>\n",
       "      <td>[former, pittsburgh, pirate, pitcher, felipe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[epc, softball, champion, finally, it’s, freedom]</td>\n",
       "      <td>[thursday, night, freedom, played, first, soft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                              Title  \\\n",
       "0        sports  [wizard, complete, turnaround, rout, pacer, cl...   \n",
       "1        sports  [falcon, rookie, kyle, pitt, invited, tight, e...   \n",
       "2        sports  [albert, pujols, go, deep, first, home, run, d...   \n",
       "3        sports  [felipe, vazquez, former, pittsburgh, pirate, ...   \n",
       "4        sports  [epc, softball, champion, finally, it’s, freedom]   \n",
       "\n",
       "                                         Description  \n",
       "0  [six, week, falling, one, thousand, seven, hun...  \n",
       "1  [falcon, rookie, tight, end, kyle, pitt, alrea...  \n",
       "2  [solid, start, dodger, blue, 10time, allstar, ...  \n",
       "3  [former, pittsburgh, pirate, pitcher, felipe, ...  \n",
       "4  [thursday, night, freedom, played, first, soft...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json',''))\n",
    "mac = '/'\n",
    "# windows = '\\\\'\n",
    "\n",
    "#dict of json files from bing news api\n",
    "dict = {'sports':[datetime.today().strftime('%Y%m%d')+'_Sports.json'],\n",
    "        'world':[datetime.today().strftime('%Y%m%d')+'_World.json'],\n",
    "        'business':[datetime.today().strftime('%Y%m%d')+'_Business.json'],\n",
    "        'science_and_technology':[datetime.today().strftime('%Y%m%d')+'_Science.json']}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "#iterates through each json file and stores as a dataframe in a list\n",
    "for k, v in dict.items():\n",
    "    for i in v:\n",
    "        init_df = pd.read_json(data_file_path+mac+i)\n",
    "        df = json_normalize(init_df['value'])\n",
    "        df = df[['name','description']]\n",
    "        df.insert(0, 'News Category', k)\n",
    "        df_list.append(df)\n",
    "        df.head()\n",
    "\n",
    "#concatenates list of dataframes into one dataframe\n",
    "data = pd.concat(df_list,axis=0)\n",
    "#renames column title to match other data\n",
    "data = data.rename(columns={'name':'Title', 'description': 'Description'})\n",
    "data.shape\n",
    "data.head()\n",
    "\n",
    "#Below is Fengling's code unedited except adding comments\n",
    "#Remove Punctuation and Stopwords\n",
    "data['Title'] = data['Title'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "data['Description'] = data['Description'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "\n",
    "\n",
    "def convert_num_to_word(words):\n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w.isnumeric():\n",
    "            result.extend(map(lambda x: x.lower(),word(w).split()))\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].str.split().apply(convert_num_to_word)\n",
    "data['Description'] = data['Description'].str.split().apply(convert_num_to_word)\n",
    "\n",
    "\n",
    "def remove_stopword(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_stopword)\n",
    "data['Description'] = data['Description'].apply(remove_stopword)\n",
    "\n",
    "\n",
    "def remove_single_character(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_single_character)\n",
    "data['Description'] = data['Description'].apply(remove_single_character)\n",
    "\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "#this groups words based on their lemma ex: walk v walked v walking\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(lemmatization)\n",
    "data['Description'] = data['Description'].apply(lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "nltk.download()\n",
    "from textblob import TextBlob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Documents'] = data['Title'] + data['Description']\n",
    "data.drop(['Title','Description'],axis=1,inplace=True)\n",
    "data['Documents'] = data['Documents'].apply(lambda x: ' '.join(x))\n",
    "data['Documents'] = data['Documents'].apply(lambda x: x.replace(\"\\'\",\"\").replace(',','').replace(']','').replace('[',''))\n",
    "data['Word Count'] = data['Documents'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "data['Noun Phrases'] = data['Documents'].apply(lambda x: len(TextBlob(x).noun_phrases))\n",
    "data['Tags'] = data['Documents'].apply(lambda t: collections.Counter(tag for word,tag in TextBlob(t).tags))\n",
    "\n",
    "# pos tag list https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "\n",
    "data['Noun Count'] = data['Tags'].apply(lambda d: d.get('NN',0)+d.get('NNS',0)+d.get('NNP',0)+d.get('NNPS',0))\n",
    "data['Adjective Count'] = data['Tags'].apply(lambda d: d.get('JJ',0)+d.get('JJR',0)+d.get('JJS',0))\n",
    "data['Verb Count'] = data['Tags'].apply(lambda d: d.get('VB',0)+d.get('VBD',0)+d.get('VBG',0)+d.get('VBN',0)+d.get('VBP',0)+d.get('VBZ',0))\n",
    "data.drop(['Tags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[wizard, complete, turnaround, rout, pacer, cl...</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[falcon, rookie, kyle, pitt, invited, tight, e...</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[albert, pujols, go, deep, first, home, run, d...</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[felipe, vazquez, former, pittsburgh, pirate, ...</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[epc, softball, champion, finally, it’s, freed...</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                          Documents  \\\n",
       "0        sports  [wizard, complete, turnaround, rout, pacer, cl...   \n",
       "1        sports  [falcon, rookie, kyle, pitt, invited, tight, e...   \n",
       "2        sports  [albert, pujols, go, deep, first, home, run, d...   \n",
       "3        sports  [felipe, vazquez, former, pittsburgh, pirate, ...   \n",
       "4        sports  [epc, softball, champion, finally, it’s, freed...   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          28             3          12                5           3  \n",
       "1          30             7          19                5           4  \n",
       "2          24             5          12                4           5  \n",
       "3          27             6          14                7           4  \n",
       "4          29             5          12                5           7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents'] = data['Documents'].str.split(\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time \n",
    "import multiprocessing\n",
    "import logging\n",
    "#logger helped identify issues and troubleshoot errors\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:39: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-05-20T22:21:39.165692', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# initializing word2vec model\n",
    "#shallow neural network model\n",
    "model = Word2Vec(min_count=2, #reduced min count for real time data\n",
    "                     window=2, # window size for context \n",
    "                     vector_size=100,  # no of features \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:39: collecting all words and their counts\n",
      "INFO - 22:21:39: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:21:39: collected 864 word types from a corpus of 1320 raw words and 48 sentences\n",
      "INFO - 22:21:39: Creating a fresh vocabulary\n",
      "INFO - 22:21:39: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 260 unique words (30.09259259259259%% of original 864, drops 604)', 'datetime': '2021-05-20T22:21:39.200238', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 22:21:39: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 716 word corpus (54.24242424242424%% of original 1320, drops 604)', 'datetime': '2021-05-20T22:21:39.200899', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 22:21:39: deleting the raw counts dictionary of 864 items\n",
      "INFO - 22:21:39: sample=6e-05 downsamples 260 most-common words\n",
      "INFO - 22:21:39: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 98.12124393725027 word corpus (13.7%% of prior 716)', 'datetime': '2021-05-20T22:21:39.205487', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 22:21:39: estimated required memory for 260 words and 100 dimensions: 338000 bytes\n",
      "INFO - 22:21:39: resetting layer weights\n",
      "INFO - 22:21:39: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-05-20T22:21:39.211537', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# build vocabulary  and learns word associations in text\n",
    "model.build_vocab(data['Documents'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:39: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 260 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-05-20T22:21:39.218344', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 1 : training on 1320 raw words (92 effective words) took 0.0s, 20927 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 2 : training on 1320 raw words (81 effective words) took 0.0s, 17884 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 3 : training on 1320 raw words (80 effective words) took 0.0s, 17229 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 4 : training on 1320 raw words (108 effective words) took 0.0s, 17861 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 5 : training on 1320 raw words (102 effective words) took 0.0s, 19171 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 6 : training on 1320 raw words (107 effective words) took 0.0s, 18638 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 7 : training on 1320 raw words (103 effective words) took 0.0s, 15280 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 8 : training on 1320 raw words (106 effective words) took 0.0s, 18012 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 9 : training on 1320 raw words (92 effective words) took 0.0s, 15103 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 10 : training on 1320 raw words (109 effective words) took 0.0s, 15560 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 11 : training on 1320 raw words (73 effective words) took 0.0s, 12129 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 12 : training on 1320 raw words (109 effective words) took 0.0s, 18679 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 13 : training on 1320 raw words (108 effective words) took 0.0s, 16434 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 14 : training on 1320 raw words (93 effective words) took 0.0s, 15380 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 15 : training on 1320 raw words (104 effective words) took 0.0s, 16864 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 16 : training on 1320 raw words (99 effective words) took 0.0s, 15340 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 17 : training on 1320 raw words (97 effective words) took 0.0s, 13277 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 18 : training on 1320 raw words (99 effective words) took 0.0s, 14019 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 19 : training on 1320 raw words (85 effective words) took 0.0s, 9135 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 20 : training on 1320 raw words (104 effective words) took 0.0s, 11731 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 21 : training on 1320 raw words (91 effective words) took 0.0s, 6866 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 22 : training on 1320 raw words (93 effective words) took 0.0s, 7982 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 23 : training on 1320 raw words (107 effective words) took 0.0s, 10205 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 24 : training on 1320 raw words (97 effective words) took 0.0s, 11198 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 25 : training on 1320 raw words (102 effective words) took 0.0s, 14357 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 26 : training on 1320 raw words (104 effective words) took 0.0s, 13242 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 27 : training on 1320 raw words (105 effective words) took 0.0s, 13495 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 28 : training on 1320 raw words (91 effective words) took 0.0s, 12424 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 29 : training on 1320 raw words (92 effective words) took 0.0s, 11314 effective words/s\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:39: EPOCH - 30 : training on 1320 raw words (95 effective words) took 0.0s, 11956 effective words/s\n",
      "INFO - 22:21:39: Word2Vec lifecycle event {'msg': 'training on 39600 raw words (2928 effective words) took 0.4s, 6812 effective words/s', 'datetime': '2021-05-20T22:21:39.649051', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#train word2vec model \n",
    "model.train(data['Documents'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all headlines \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    feature_vecs = np.zeros((len(words),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for word in words:\n",
    "        feature_vecs[counter] = make_feature_vec(word, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the features by calculating average of the word vectors\n",
    "word2vec = get_avg_feature_vecs(data['Documents'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.297017e-04</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.271050e-04</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001793</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>-0.006047</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.129143e-03</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>-0.000751</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>-0.003625</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.183281e-03</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.001867</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.003454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2.657529e-07</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  Word Count  Noun Phrases  Noun Count  Adjective Count  \\\n",
       "0        sports          28             3          12                5   \n",
       "1        sports          30             7          19                5   \n",
       "2        sports          24             5          12                4   \n",
       "3        sports          27             6          14                7   \n",
       "4        sports          29             5          12                5   \n",
       "\n",
       "   Verb Count             0         1         2         3  ...        90  \\\n",
       "0           3 -9.297017e-04 -0.001585 -0.000862  0.002884  ...  0.003431   \n",
       "1           4 -2.271050e-04 -0.001200 -0.001793  0.001442  ...  0.005584   \n",
       "2           5 -2.129143e-03  0.001486 -0.000751 -0.000340  ...  0.003299   \n",
       "3           4  1.183281e-03  0.001479  0.000521 -0.001686  ...  0.003005   \n",
       "4           7  2.657529e-07  0.002532  0.002676  0.003965  ...  0.002085   \n",
       "\n",
       "         91        92        93        94        95        96        97  \\\n",
       "0  0.004484  0.000868  0.000514  0.003661  0.001792  0.002909  0.000749   \n",
       "1  0.002658 -0.000201  0.001161  0.004840  0.005009  0.001653 -0.006047   \n",
       "2  0.002329  0.000945 -0.002247  0.002309  0.000604  0.003786 -0.003625   \n",
       "3  0.001856  0.001275  0.000857  0.003049  0.002238  0.000035 -0.001867   \n",
       "4  0.000960  0.001430  0.000224  0.006805  0.002485  0.000762  0.000269   \n",
       "\n",
       "         98        99  \n",
       "0  0.002014  0.002302  \n",
       "1  0.001283  0.001741  \n",
       "2  0.000372  0.000647  \n",
       "3  0.000474  0.003454  \n",
       "4  0.001942  0.000379  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = pd.DataFrame(word2vec)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "w2v.reset_index(drop=True, inplace=True)\n",
    "#df = pd.concat([df1, df2], axis=1)\n",
    "w2v = pd.concat([data[['News Category','Word Count','Noun Phrases','Noun Count',\n",
    "                                         'Adjective Count','Verb Count']],w2v],axis=1)\n",
    "\n",
    "# remove instances in test set that could not be represented as feature vectors\n",
    "w2v.dropna(inplace=True)\n",
    "w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkl_Filename = \"Pickle_RL_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LR Model on Real-Time Data with W2V Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe \n",
    "X_test = w2v.drop(['News Category'],axis=1) \n",
    "# y series\n",
    "y_test = w2v['News Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.25 0.25 0.0625 0.1 0.5\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.to_numpy()\n",
    "y_pred = Pickled_LR_Model.predict(X_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test,y_pred)\n",
    "lr_recall = recall_score(y_test,y_pred,average='macro')\n",
    "lr_precision = precision_score(y_test,y_pred,average='macro')\n",
    "lr_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "lr_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')\n",
    "\n",
    "print(\"Logistic Regression: \",lr_acc,lr_recall,lr_precision,lr_f1,lr_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
